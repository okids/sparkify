{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify Project Workspace\n",
    "This workspace contains a tiny subset (128MB) of the full dataset available (12GB). Feel free to use this workspace to build your project, or to explore a smaller subset with Spark before deploying your cluster on the cloud. Instructions for setting up your Spark cluster is included in the last lesson of the Extracurricular Spark Course content.\n",
    "\n",
    "You can follow the steps below to guide your data analysis and model building portion of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'proxyUser': 'user_okids94', 'conf': {'spark.pyspark.python': 'python3', 'spark.pyspark.virtualenv.enabled': 'true', 'spark.pyspark.virtualenv.type': 'native', 'spark.pyspark.virtualenv.bin.path': '/usr/bin/virtualenv'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1589627906482_0001</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-2-207.eu-west-1.compute.internal:20888/proxy/application_1589627906482_0001/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-12-72.eu-west-1.compute.internal:8042/node/containerlogs/container_1589627906482_0001_01_000001/livy\">Link</a></td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%info\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4de641507b247adb69124c09fccb521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1589627906482_0002</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-2-207.eu-west-1.compute.internal:20888/proxy/application_1589627906482_0002/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-12-72.eu-west-1.compute.internal:8042/node/containerlogs/container_1589627906482_0002_01_000002/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                    Version\n",
      "-------------------------- -------\n",
      "beautifulsoup4             4.9.0  \n",
      "boto                       2.49.0 \n",
      "jmespath                   0.9.5  \n",
      "lxml                       4.5.0  \n",
      "mysqlclient                1.4.2  \n",
      "nltk                       3.4.5  \n",
      "nose                       1.3.4  \n",
      "numpy                      1.16.5 \n",
      "pip                        9.0.1  \n",
      "py-dateutil                2.2    \n",
      "python37-sagemaker-pyspark 1.3.0  \n",
      "pytz                       2019.3 \n",
      "PyYAML                     5.3.1  \n",
      "setuptools                 28.8.0 \n",
      "six                        1.13.0 \n",
      "soupsieve                  1.9.5  \n",
      "wheel                      0.29.0 \n",
      "windmill                   1.6"
     ]
    }
   ],
   "source": [
    "sc.list_packages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cba48d80bf8140b09829124af9bc8a2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Package already installed for current Spark context!\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/context.py\", line 1110, in install_pypi_package\n",
      "    raise ValueError(\"Package already installed for current Spark context!\")\n",
      "ValueError: Package already installed for current Spark context!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sc.install_pypi_package(\"pandas==0.25.1\") #Install pandas version 0.25.1 \n",
    "sc.install_pypi_package(\"matplotlib\", \"https://pypi.org/simple\") #Install matplotlib from given PyPI repository\n",
    "sc.install_pypi_package(\"httpagentparser\")\n",
    "sc.install_pypi_package(\"seaborn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca79fea55f534cc488a4cb391a3e702d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "from pyspark.sql.functions import desc,asc,col,sum as Fsum,udf\n",
    "\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler,StringIndexer,StandardScaler\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression,GBTClassifier,NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "import datetime\n",
    "import httpagentparser\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# %matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "296a6deb079e4534974adbdecb5feb6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a Spark session\n",
    "\n",
    "# Create spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Sparkify\") \\\n",
    "    .getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Clean Dataset\n",
    "In this workspace, the mini-dataset file is `mini_sparkify_event_data.json`. Load and clean the dataset, checking for invalid or missing data - for example, records without userids or sessionids. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "786107b40c2c469b8e7a96b3aea05366",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read in full sparkify dataset\n",
    "event_data = \"s3n://udacity-dsnd/sparkify/sparkify_event_data.json\"\n",
    "df = spark.read.json(event_data)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, artist: string, auth: string, firstName: string, gender: string, itemInSession: string, lastName: string, length: string, level: string, location: string, method: string, page: string, registration: string, sessionId: string, song: string, status: string, ts: string, userAgent: string, userId: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "572bc76d60654a5a93f57b7e9b07706a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of dataset with null artist :  5408927\n",
      "number of dataset with null userid :  0\n",
      "number of dataset with null sessionid :  0\n",
      "number of dataset with empty userid :  0\n",
      "number of dataset with empty sessionid :  0"
     ]
    }
   ],
   "source": [
    "print('number of dataset with null artist : ' ,df.where(col(\"artist\").isNull()).count())\n",
    "print('number of dataset with null userid : ' ,df.where(col(\"userId\").isNull()).count())\n",
    "print('number of dataset with null sessionid : ' ,df.where(col(\"sessionId\").isNull()).count())\n",
    "print('number of dataset with empty userid : ' ,df.where(col(\"userId\") == '').count())\n",
    "print('number of dataset with empty sessionid : ' ,df.where(col(\"sessionid\") == '').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dd55b11ebad4394946b2dc36d75c824",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#filter empty string\n",
    "df_cleaned = df.filter(df.userId != '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b17e3a10d5104da393d35f6e5eaedcad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all data :  26259199\n",
      "non duplicate data :  26259199\n",
      "No duplicate"
     ]
    }
   ],
   "source": [
    "#Check duplicates\n",
    "print(\"all data : \",df_cleaned.count())\n",
    "print(\"non duplicate data : \", df_cleaned.drop_duplicates().count())\n",
    "if df_cleaned.count()==df_cleaned.drop_duplicates().count():\n",
    "    print('No duplicate')\n",
    "else:\n",
    "    print('duplicate dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "When you're working with the full dataset, perform EDA by loading a small subset of the data and doing basic manipulations within Spark. In this workspace, you are already provided a small subset of data you can explore.\n",
    "\n",
    "### Define Churn\n",
    "\n",
    "Once you've done some preliminary analysis, create a column `Churn` to use as the label for your model. I suggest using the `Cancellation Confirmation` events to define your churn, which happen for both paid and free users. As a bonus task, you can also look into the `Downgrade` events.\n",
    "\n",
    "### Explore Data\n",
    "Once you've defined churn, perform some exploratory data analysis to observe the behavior for users who stayed vs users who churned. You can start by exploring aggregates on these two groups of users, observing how much of a specific action they experienced per a certain time unit or number of songs played."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "884116cc1188436e9c9271a900726ca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|gender|   count|\n",
      "+------+--------+\n",
      "|  null|  778479|\n",
      "|     M|13299562|\n",
      "|     F|12181158|\n",
      "+------+--------+"
     ]
    }
   ],
   "source": [
    "#Distribution of gender\n",
    "df_cleaned.groupby('gender').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2692f7572484432fa40afc480e4932ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|           length|\n",
      "+-------+-----------------+\n",
      "|  count|         20850272|\n",
      "|   mean|248.7254329674883|\n",
      "| stddev|97.28710387078065|\n",
      "|    min|            0.522|\n",
      "|    max|       3024.66567|\n",
      "+-------+-----------------+"
     ]
    }
   ],
   "source": [
    "#Statistical result of song length\n",
    "df_cleaned.select('length').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a51d12f87a64717bc79d3c8c01807c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|                page|  count|\n",
      "+--------------------+-------+\n",
      "| Submit Registration|    401|\n",
      "|            Register|    802|\n",
      "|              Cancel|   5003|\n",
      "|Cancellation Conf...|   5003|\n",
      "|    Submit Downgrade|   6494|\n",
      "|      Submit Upgrade|  15135|\n",
      "|               Error|  25962|\n",
      "|       Save Settings|  29516|\n",
      "|             Upgrade|  50507|\n",
      "|               About|  92759|\n",
      "|            Settings| 147074|\n",
      "|                Help| 155100|\n",
      "|           Downgrade| 184240|\n",
      "|         Thumbs Down| 239212|\n",
      "|              Logout| 296005|\n",
      "|               Login| 296350|\n",
      "|          Add Friend| 381664|\n",
      "|         Roll Advert| 385212|\n",
      "|     Add to Playlist| 597921|\n",
      "|           Thumbs Up|1151465|\n",
      "+--------------------+-------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "df_cleaned.groupby('page').count().sort('count').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57a8ea095e4d48b4bd1201d5d131e8c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|status|   count|\n",
      "+------+--------+\n",
      "|   404|   25962|\n",
      "|   307| 2421245|\n",
      "|   200|23811992|\n",
      "+------+--------+"
     ]
    }
   ],
   "source": [
    "df_cleaned.groupby('status').count().sort('count').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "292c5001383f495e88134063eeb1847a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_cleaned.createOrReplaceTempView(\"event\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4caf827deb984ffdb1f579d1677a1416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------------+\n",
      "|gender|count(DISTINCT userId)|\n",
      "+------+----------------------+\n",
      "|     M|                  2656|\n",
      "|     F|                  2347|\n",
      "+------+----------------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT gender, COUNT(DISTINCT userId) \n",
    "             FROM event \n",
    "             WHERE page = 'Cancellation Confirmation' GROUP BY 1 \n",
    "          \"\"\").show(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total 5003 users is churning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "267c8246e8db4fcfb2b4dc535e9fd499",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|                page|count(1)|\n",
      "+--------------------+--------+\n",
      "|       Save Settings|       1|\n",
      "|     Add to Playlist|       2|\n",
      "|                Help|       1|\n",
      "|              Cancel|       1|\n",
      "|         Thumbs Down|       1|\n",
      "|                Home|      11|\n",
      "|         Roll Advert|       6|\n",
      "|             Upgrade|       3|\n",
      "|Cancellation Conf...|       1|\n",
      "|            NextSong|     123|\n",
      "|           Thumbs Up|       7|\n",
      "|              Logout|       3|\n",
      "|            Settings|       1|\n",
      "+--------------------+--------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT page, COUNT(*) FROM event WHERE userId in (\n",
    "             SELECT userId \n",
    "             FROM event \n",
    "             WHERE page = 'Cancellation Confirmation' LIMIT 1) GROUP BY 1 \n",
    "          \"\"\").show(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of list of page that churn user visited. If you see on the 'NextSong' page, this user played 123 songs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b83796c087654d62be7bdd59018a0e3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Create new view with churn columns\n",
    "event_with_churn = spark.sql(\"\"\"\n",
    "SELECT e.*, COALESCE(churn,0) AS churn\n",
    "FROM event e\n",
    "LEFT JOIN(\n",
    "        SELECT userid,1 AS churn\n",
    "         FROM event \n",
    "         WHERE page = 'Cancellation Confirmation'\n",
    "         GROUP BY 1\n",
    "         ) USING (userid)\n",
    "          \"\"\")\n",
    "event_with_churn.createOrReplaceTempView(\"event_with_churn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0cdb31d0a34429a8d170721e7310bec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------------+\n",
      "|churn|count(DISTINCT userId)|\n",
      "+-----+----------------------+\n",
      "|    1|                  5003|\n",
      "|    0|                 17275|\n",
      "+-----+----------------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" SELECT churn, COUNT(DISTINCT userId) FROM\n",
    "event_with_churn\n",
    "GROUP BY 1\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb2ab286a1704e278fee40f1c6522757",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+-----------------------+------------------+----------------------+------------------+------------------+\n",
      "|churn|  avg_song_played|avg_song_per_day_played|median_song_played|avg_unique_song_played|      unique_ratio|          user_age|\n",
      "+-----+-----------------+-----------------------+------------------+----------------------+------------------+------------------+\n",
      "|    1|876.7291625024985|        58.727684128275|               512|     779.5772536478113| 1.124621271849701|3.7419776765812545|\n",
      "|    0|953.0533140376266|      53.24271634785819|               563|     842.7589001447178|1.1308730336445798| 5.231680025687944|\n",
      "+-----+-----------------+-----------------------+------------------+----------------------+------------------+------------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" \n",
    "SELECT churn, \n",
    "       AVG(cnt_song) avg_song_played,\n",
    "       AVG(cnt_song/cnt_day_played) avg_song_per_day_played,\n",
    "       percentile_approx(cnt_song, 0.5) median_song_played,\n",
    "       AVG(cnt_unique_song)  avg_unique_song_played,\n",
    "       AVG(cnt_song)/AVG(cnt_unique_song) AS unique_ratio,\n",
    "       AVG(ts-registration)/(1000*3600*365*24) AS user_age\n",
    " FROM\n",
    "(SELECT userId, \n",
    "        MAX(churn) AS churn, \n",
    "        SUM(CASE WHEN page = 'NextSong' THEN 1 ELSE 0 END) cnt_song,\n",
    "        COUNT(DISTINCT CASE WHEN page = 'NextSong' THEN song END) cnt_unique_song,\n",
    "        MAX(ts) ts,\n",
    "        COUNT(DISTINCT CASE WHEN page = 'NextSong' THEN from_unixtime(ts/1000, 'yyyy-MM-dd') ELSE 0 END) cnt_day_played,\n",
    "        MAX(registration) registration\n",
    "   FROM event_with_churn\n",
    " GROUP BY 1) GROUP BY 1 \n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On average, churn user played less song compared to active user. They also have less in user_age (3.7 years compare to 5.2 years). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80c7c30461c44a15af51fb4df8ab1d53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "|level|count(1)|\n",
      "+-----+--------+\n",
      "| free| 5663371|\n",
      "| paid|20595828|\n",
      "+-----+--------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT level, COUNT(*) FROM event_with_churn GROUP BY 1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30d220ed2ad54a388cf24eae830d1ea9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_song_perday = spark.sql(\"\"\" \n",
    "SELECT churn, \n",
    "       cnt_song/cnt_day_played num_song_perday\n",
    " FROM\n",
    "(SELECT userId, \n",
    "        MAX(churn) AS churn, \n",
    "        SUM(CASE WHEN page = 'NextSong' THEN 1 ELSE 0 END) cnt_song,\n",
    "        MAX(ts) ts,\n",
    "        COUNT(DISTINCT CASE WHEN page = 'NextSong' THEN from_unixtime(ts/1000, 'yyyy-MM-dd') ELSE 0 END) cnt_day_played,\n",
    "        MAX(registration) registration\n",
    "   FROM event_with_churn\n",
    " GROUP BY 1) \n",
    "\"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41d494da8dc2470483a6ea639d5cbe4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<matplotlib.axes._subplots.AxesSubplot object at 0x7f98d9e5d990>"
     ]
    }
   ],
   "source": [
    "df_song_perday[df_song_perday.churn == 1].num_song_perday.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29cd3cff5e0d439391902d2194167bcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<matplotlib.axes._subplots.AxesSubplot object at 0x7f98d9e5d990>"
     ]
    }
   ],
   "source": [
    "df_song_perday[df_song_perday.churn == 0].num_song_perday.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Churn user has right skewed distribution in terms of song per day, which means they play less song per day. On the other hand, active users is more similar to normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Once you've familiarized yourself with the data, build out the features you find promising to train your model on. To work with the full dataset, you can follow the following steps.\n",
    "- Write a script to extract the necessary features from the smaller subset of data\n",
    "- Ensure that your script is scalable, using the best practices discussed in Lesson 3\n",
    "- Try your script on the full data set, debugging your script if necessary\n",
    "\n",
    "If you are working in the classroom workspace, you can just extract features based on the small subset of data contained here. Be sure to transfer over this work to the larger dataset when you work on your Spark cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2677d1e1fbfa49b98ee3e47589aa6bf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Create dummy column\n",
    "def create_dummy_columns(col_name,df):\n",
    "    '''\n",
    "    Input :\n",
    "    - col_name = name of the column that is used to generate dummy variables\n",
    "    - df = spark df\n",
    "    Output :\n",
    "    - ndf = new spark df with all dummy variables from selected column\n",
    "    '''\n",
    "    elements = df.select(col_name).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "    exprs = [F.when(F.col(col_name) == element, 1).otherwise(0).alias(element)\n",
    "         for element in elements]\n",
    "    df_with_dummy = df.select(\"*\", *exprs)\n",
    "    \n",
    "    #drop original column\n",
    "    ndf = df_with_dummy.drop(col_name)\n",
    "    return ndf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3fc8ecbc03545b59670f9ee39316b21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_with_dummy_pages = create_dummy_columns('page',event_with_churn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb9a0c2c940f4eeebe9815e605a4285e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_browser(x):\n",
    "    try :\n",
    "        r = httpagentparser.detect(x)['browser']['name']\n",
    "    except:\n",
    "        r = ''\n",
    "    return r\n",
    "    \n",
    "def get_platform(x):\n",
    "    try :\n",
    "        r = httpagentparser.detect(x)['platform']['name']\n",
    "    except:\n",
    "        r = ''\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "497660f6b6d4479bb61e34ee7d59478c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function get_platform at 0x7f009c28ad40>"
     ]
    }
   ],
   "source": [
    "#browser\n",
    "spark.udf.register(\"get_browser\", get_browser)\n",
    "\n",
    "#Platform\n",
    "spark.udf.register(\"get_platform\", get_platform)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "811914a7244249fabe8a72561dbcd72a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o373.showString.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 13 in stage 156.0 failed 4 times, most recent failure: Lost task 13.3 in stage 156.0 (TID 4431, ip-172-31-12-72.eu-west-1.compute.internal, executor 16): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1589627906482_0001/container_1589627906482_0001_01_000033/pyspark.zip/pyspark/worker.py\", line 377, in main\n",
      "    process()\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1589627906482_0001/container_1589627906482_0001_01_000033/pyspark.zip/pyspark/worker.py\", line 372, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1589627906482_0001/container_1589627906482_0001_01_000033/pyspark.zip/pyspark/serializers.py\", line 352, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1589627906482_0001/container_1589627906482_0001_01_000033/pyspark.zip/pyspark/serializers.py\", line 142, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1589627906482_0001/container_1589627906482_0001_01_000033/pyspark.zip/pyspark/serializers.py\", line 341, in _batched\n",
      "    for item in iterator:\n",
      "  File \"<string>\", line 1, in <lambda>\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1589627906482_0001/container_1589627906482_0001_01_000033/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n",
      "    return lambda *a: f(*a)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1589627906482_0001/container_1589627906482_0001_01_000033/pyspark.zip/pyspark/util.py\", line 113, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"<stdin>\", line 2, in <lambda>\n",
      "KeyError: 'browser'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.agg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:585)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:188)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2043)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2031)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2030)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2030)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:967)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:967)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:967)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2264)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2213)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2202)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.checkNoFailures(AdaptiveExecutor.scala:146)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.doRun(AdaptiveExecutor.scala:88)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.tryRunningAndGetFuture(AdaptiveExecutor.scala:66)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.execute(AdaptiveExecutor.scala:57)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$$anonfun$finalPhysicalPlan$1.apply(AdaptiveSparkPlanExec.scala:128)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$$anonfun$finalPhysicalPlan$1.apply(AdaptiveSparkPlanExec.scala:127)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:777)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.finalPhysicalPlan(AdaptiveSparkPlanExec.scala:127)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:134)\n",
      "\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3395)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2552)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2552)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.org$apache$spark$sql$execution$SQLExecution$$executeQuery$1(SQLExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1$$anonfun$apply$1.apply(SQLExecution.scala:94)\n",
      "\tat org.apache.spark.sql.execution.QueryExecutionMetrics$.withMetrics(QueryExecutionMetrics.scala:141)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.org$apache$spark$sql$execution$SQLExecution$$withMetrics(SQLExecution.scala:178)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:200)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:92)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n",
      "\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2552)\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2766)\n",
      "\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:255)\n",
      "\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:292)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1589627906482_0001/container_1589627906482_0001_01_000033/pyspark.zip/pyspark/worker.py\", line 377, in main\n",
      "    process()\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1589627906482_0001/container_1589627906482_0001_01_000033/pyspark.zip/pyspark/worker.py\", line 372, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1589627906482_0001/container_1589627906482_0001_01_000033/pyspark.zip/pyspark/serializers.py\", line 352, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1589627906482_0001/container_1589627906482_0001_01_000033/pyspark.zip/pyspark/serializers.py\", line 142, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1589627906482_0001/container_1589627906482_0001_01_000033/pyspark.zip/pyspark/serializers.py\", line 341, in _batched\n",
      "    for item in iterator:\n",
      "  File \"<string>\", line 1, in <lambda>\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1589627906482_0001/container_1589627906482_0001_01_000033/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n",
      "    return lambda *a: f(*a)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1589627906482_0001/container_1589627906482_0001_01_000033/pyspark.zip/pyspark/util.py\", line 113, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"<stdin>\", line 2, in <lambda>\n",
      "KeyError: 'browser'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.agg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:585)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:188)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 381, in show\n",
      "    print(self._jdf.showString(n, 20, vertical))\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o373.showString.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 13 in stage 156.0 failed 4 times, most recent failure: Lost task 13.3 in stage 156.0 (TID 4431, ip-172-31-12-72.eu-west-1.compute.internal, executor 16): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1589627906482_0001/container_1589627906482_0001_01_000033/pyspark.zip/pyspark/worker.py\", line 377, in main\n",
      "    process()\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1589627906482_0001/container_1589627906482_0001_01_000033/pyspark.zip/pyspark/worker.py\", line 372, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1589627906482_0001/container_1589627906482_0001_01_000033/pyspark.zip/pyspark/serializers.py\", line 352, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1589627906482_0001/container_1589627906482_0001_01_000033/pyspark.zip/pyspark/serializers.py\", line 142, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1589627906482_0001/container_1589627906482_0001_01_000033/pyspark.zip/pyspark/serializers.py\", line 341, in _batched\n",
      "    for item in iterator:\n",
      "  File \"<string>\", line 1, in <lambda>\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1589627906482_0001/container_1589627906482_0001_01_000033/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n",
      "    return lambda *a: f(*a)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1589627906482_0001/container_1589627906482_0001_01_000033/pyspark.zip/pyspark/util.py\", line 113, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"<stdin>\", line 2, in <lambda>\n",
      "KeyError: 'browser'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.agg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:585)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:188)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2043)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2031)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2030)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2030)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:967)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:967)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:967)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2264)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2213)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2202)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.checkNoFailures(AdaptiveExecutor.scala:146)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.doRun(AdaptiveExecutor.scala:88)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.tryRunningAndGetFuture(AdaptiveExecutor.scala:66)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.execute(AdaptiveExecutor.scala:57)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$$anonfun$finalPhysicalPlan$1.apply(AdaptiveSparkPlanExec.scala:128)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$$anonfun$finalPhysicalPlan$1.apply(AdaptiveSparkPlanExec.scala:127)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:777)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.finalPhysicalPlan(AdaptiveSparkPlanExec.scala:127)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:134)\n",
      "\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3395)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2552)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2552)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.org$apache$spark$sql$execution$SQLExecution$$executeQuery$1(SQLExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1$$anonfun$apply$1.apply(SQLExecution.scala:94)\n",
      "\tat org.apache.spark.sql.execution.QueryExecutionMetrics$.withMetrics(QueryExecutionMetrics.scala:141)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.org$apache$spark$sql$execution$SQLExecution$$withMetrics(SQLExecution.scala:178)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:200)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:92)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n",
      "\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2552)\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2766)\n",
      "\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:255)\n",
      "\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:292)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1589627906482_0001/container_1589627906482_0001_01_000033/pyspark.zip/pyspark/worker.py\", line 377, in main\n",
      "    process()\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1589627906482_0001/container_1589627906482_0001_01_000033/pyspark.zip/pyspark/worker.py\", line 372, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1589627906482_0001/container_1589627906482_0001_01_000033/pyspark.zip/pyspark/serializers.py\", line 352, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1589627906482_0001/container_1589627906482_0001_01_000033/pyspark.zip/pyspark/serializers.py\", line 142, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1589627906482_0001/container_1589627906482_0001_01_000033/pyspark.zip/pyspark/serializers.py\", line 341, in _batched\n",
      "    for item in iterator:\n",
      "  File \"<string>\", line 1, in <lambda>\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1589627906482_0001/container_1589627906482_0001_01_000033/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n",
      "    return lambda *a: f(*a)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1589627906482_0001/container_1589627906482_0001_01_000033/pyspark.zip/pyspark/util.py\", line 113, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"<stdin>\", line 2, in <lambda>\n",
      "KeyError: 'browser'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.agg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:585)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:188)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check unique combination\n",
    "spark.sql(\"SELECT get_browser(userAgent), get_platform(userAgent) FROM event_with_churn GROUP BY 1,2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "707c9b11597746b4b71f70c11ebfeb9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Crafting session level feature\n",
    "session_feature = spark.sql(\"\"\"\n",
    "SELECT userId, \n",
    "       AVG(hour_per_session)             AS avg_hour_per_session,\n",
    "       AVG(unique_song_per_session)      AS avg_unique_song_per_session,\n",
    "       AVG(unique_artist_per_session)    AS avg_unique_artist_per_session,\n",
    "       AVG(cnt_song_played_persession)   AS avg_song_played_persession\n",
    "FROM (       \n",
    "SELECT userId,\n",
    "       sessionId,\n",
    "       (MAX(ts) -MIN(ts))/(1000*3600) AS hour_per_session,\n",
    "       COUNT(DISTINCT song) unique_song_per_session,\n",
    "       COUNT(DISTINCT artist) unique_artist_per_session,\n",
    "       SUM(CASE WHEN page = 'NextSong' THEN 1 ELSE 0 END) AS cnt_song_played_persession\n",
    "FROM event_with_churn \n",
    "GROUP BY 1,2)\n",
    "GROUP BY 1\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4346f6a9a755458aaf2ddfa348d1e69a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+---------------------------+-----------------------------+--------------------------+\n",
      "| userId|avg_hour_per_session|avg_unique_song_per_session|avg_unique_artist_per_session|avg_song_played_persession|\n",
      "+-------+--------------------+---------------------------+-----------------------------+--------------------------+\n",
      "|1338783|  7.9227951388888895|                     101.75|                      95.8125|                  104.5625|\n",
      "|1271218|   6.910691721132898|          91.76470588235294|            87.07843137254902|         93.70588235294117|\n",
      "|1396135|   7.612685185185185|         105.88888888888889|                        101.0|        108.22222222222223|\n",
      "|1690101|   4.364001501501501|         63.513513513513516|            61.37837837837838|         64.37837837837837|\n",
      "|1676292|  10.208055555555553|         144.72727272727272|           132.45454545454547|         149.0909090909091|\n",
      "|1380035|  5.4878806584362145|          71.25925925925925|            69.14814814814815|         72.07407407407408|\n",
      "|1732896|   7.269055555555555|                      101.6|                         97.8|                     102.8|\n",
      "|1440693|  1.3508854166666666|                    20.6875|                      20.4375|                    20.875|\n",
      "|1507765|   4.745333333333333|                       68.4|                         65.8|                      69.4|\n",
      "|1853227|  3.2821450617283947|         47.888888888888886|            46.05555555555556|         48.77777777777778|\n",
      "|1242455|   6.856421568627452|          96.17647058823529|                         91.0|         98.47058823529412|\n",
      "|1010669|   9.027962962962963|         121.47619047619048|            115.0952380952381|         124.0952380952381|\n",
      "|1927014|   5.690260942760942|          76.27272727272727|            72.54545454545455|         77.81818181818181|\n",
      "|1675930|   6.767703703703703|          96.26666666666667|            91.66666666666667|                      98.0|\n",
      "|1925919|   1.336527777777778|                       21.5|                         21.5|                      21.5|\n",
      "|1883991|   7.613842592592591|                     104.75|            96.41666666666667|        109.08333333333333|\n",
      "|1025974|   4.239823232323233|          61.81818181818182|            59.72727272727273|                      62.0|\n",
      "|1297189|  3.5607142857142855|          51.57142857142857|           50.142857142857146|                      52.0|\n",
      "|1766909|   2.090099206349206|         30.857142857142858|           30.571428571428573|        30.928571428571427|\n",
      "|1008404|   6.527443415637861|          91.01851851851852|            86.37037037037037|         92.87037037037037|\n",
      "+-------+--------------------+---------------------------+-----------------------------+--------------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "session_feature.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4187f1d227e46c8b0f31bea3e60650a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "session_feature.createOrReplaceTempView('session_feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce4a7ce69b284c5e9b27393dfcb74d31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Crafting the feature\n",
    "full_feature = spark.sql(\"\"\"\n",
    "SELECT userId,\n",
    "       churn,  \n",
    "       --Gender\n",
    "       MAX(CASE WHEN gender = 'M' THEN 1 ELSE 0 END)  AS gender_male,\n",
    "       MAX(CASE WHEN gender = 'F' THEN 1 ELSE 0 END) AS gender_female,\n",
    "       \n",
    "       -- browser\n",
    "       MAX(CASE WHEN get_browser(userAgent) = 'Chrome' THEN 1 ELSE 0 END)  AS chrome_user,\n",
    "       MAX(CASE WHEN get_browser(userAgent) = 'Firefox' THEN 1 ELSE 0 END)  AS firefox_user,\n",
    "       MAX(CASE WHEN get_browser(userAgent) = 'Safari' THEN 1 ELSE 0 END)  AS safari_user,\n",
    "       MAX(CASE WHEN get_browser(userAgent) LIKE '%Microsoft%' THEN 1 ELSE 0 END)  AS ie_user,\n",
    "       \n",
    "       --platform\n",
    "       MAX(CASE WHEN get_platform(userAgent) = 'Linux' THEN 1 ELSE 0 END)  AS linux_user,\n",
    "       MAX(CASE WHEN get_platform(userAgent) = 'iOS' THEN 1 ELSE 0 END)  AS ios_user,\n",
    "       MAX(CASE WHEN get_platform(userAgent) = 'Mac OS' THEN 1 ELSE 0 END)  AS mac_user,\n",
    "       MAX(CASE WHEN get_platform(userAgent) = 'Windows' THEN 1 ELSE 0 END)  AS windows_user,\n",
    "       \n",
    "       \n",
    "       \n",
    "       \n",
    "       -- Paid and free session\n",
    "       COUNT(DISTINCT CASE WHEN level = 'paid' THEN sessionId ELSE 0 END) AS cnt_paid_session,\n",
    "       COUNT(DISTINCT CASE WHEN level = 'free' THEN sessionId ELSE 0 END) AS cnt_free_session,\n",
    "       \n",
    "       -- Artist and song behavior\n",
    "       COUNT(DISTINCT artist) AS cnt_unique_artist,\n",
    "       COUNT(DISTINCT song) AS cnt_unique_song,\n",
    "       AVG(length) AS average_song_length,\n",
    "       SUM(CASE WHEN page = 'NextSong' THEN 1 ELSE 0 END) AS cnt_song_played,\n",
    "    \n",
    "       -- Other behavior\n",
    "       COUNT(DISTINCT CASE WHEN page = 'NextSong' THEN from_unixtime(ts/1000, 'yyyy-MM-dd') ELSE 0 END) AS cnt_day_played,\n",
    "       COUNT(DISTINCT sessionId) AS cnt_session,\n",
    "       SUM(CASE WHEN page = 'Add Friend' THEN 1 ELSE 0 END) AS cnt_add_friend,\n",
    "       SUM(CASE WHEN page = 'Thumbs Down' THEN 1 ELSE 0 END) AS cnt_thumb_down,\n",
    "       SUM(CASE WHEN page = 'Thumbs Up' THEN 1 ELSE 0 END) AS cnt_thumbs_up,\n",
    "       SUM(CASE WHEN page = 'Home' THEN 1 ELSE 0 END) AS cnt_home,\n",
    "       SUM(CASE WHEN page = 'Logout' THEN 1 ELSE 0 END) AS cnt_logout,\n",
    "       SUM(CASE WHEN page = 'Help' THEN 1 ELSE 0 END) AS cnt_help,\n",
    "       SUM(CASE WHEN page = 'Settings' THEN 1 ELSE 0 END) AS cnt_settings,\n",
    "       SUM(CASE WHEN page = 'Submit Downgrade' THEN 1 ELSE 0 END) AS cnt_submit_downgrade,\n",
    "       SUM(CASE WHEN page = 'Submit Upgrade' THEN 1 ELSE 0 END) AS cnt_submit_upgrade,\n",
    "       SUM(CASE WHEN page = 'Downgrade' THEN 1 ELSE 0 END) AS cnt_initiate_downgrade,\n",
    "       SUM(CASE WHEN page = 'Upgrade' THEN 1 ELSE 0 END) AS cnt_initiate_upgrade,\n",
    "       SUM(CASE WHEN page = 'Error' THEN 1 ELSE 0 END) AS cnt_error,\n",
    "       MAX(ts-registration)/(1000*3600*365*24) AS user_age,\n",
    "       \n",
    "       --Combining session feature\n",
    "       MAX(sf.avg_hour_per_session)  AS avg_hour_per_session,\n",
    "       MAX(sf.avg_unique_song_per_session) AS avg_unique_song_per_session,\n",
    "       MAX(sf.avg_unique_artist_per_session) AS avg_unique_artist_per_session,\n",
    "       MAX(sf.avg_song_played_persession) AS avg_song_played_persession\n",
    "    \n",
    "FROM event_with_churn \n",
    "LEFT JOIN session_feature sf USING (userId)\n",
    "GROUP BY 1,2     \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc1e96982fc74108af41131f0661e939",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[userId: string, churn: int, gender_male: int, gender_female: int, chrome_user: int, firefox_user: int, safari_user: int, ie_user: int, linux_user: int, ios_user: int, mac_user: int, windows_user: int, cnt_paid_session: bigint, cnt_free_session: bigint, cnt_unique_artist: bigint, cnt_unique_song: bigint, average_song_length: double, cnt_song_played: bigint, cnt_day_played: bigint, cnt_session: bigint, cnt_add_friend: bigint, cnt_thumb_down: bigint, cnt_thumbs_up: bigint, cnt_home: bigint, cnt_logout: bigint, cnt_help: bigint, cnt_settings: bigint, cnt_submit_downgrade: bigint, cnt_submit_upgrade: bigint, cnt_initiate_downgrade: bigint, cnt_initiate_upgrade: bigint, cnt_error: bigint, user_age: double, avg_hour_per_session: double, avg_unique_song_per_session: double, avg_unique_artist_per_session: double, avg_song_played_persession: double]"
     ]
    }
   ],
   "source": [
    "full_feature.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "Split the full dataset into train, test, and validation sets. Test out several of the machine learning methods you learned. Evaluate the accuracy of the various models, tuning parameters as necessary. Determine your winning model based on test accuracy and report results on the validation set. Since the churned users are a fairly small subset, I suggest using F1 score as the metric to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6624bf9d96204b56b94ba30d80eaa0a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "features_col = ['gender_male','gender_female','chrome_user','firefox_user','safari_user',\n",
    "               'ie_user','linux_user','ios_user','mac_user','windows_user','cnt_paid_session',\n",
    "               'cnt_free_session','cnt_unique_artist','cnt_unique_song','average_song_length',\n",
    "               'cnt_song_played','cnt_day_played','cnt_session','cnt_add_friend','cnt_thumb_down',\n",
    "               'cnt_thumbs_up','cnt_home','cnt_logout','cnt_help','cnt_settings','cnt_submit_downgrade',\n",
    "               'cnt_submit_upgrade','cnt_initiate_downgrade','cnt_initiate_upgrade','cnt_error',\n",
    "               'user_age','avg_hour_per_session','avg_unique_song_per_session','avg_unique_artist_per_session'\n",
    "                ]\n",
    "assembler = VectorAssembler(inputCols=features_col, outputCol=\"feature_vec\")\n",
    "\n",
    "df = assembler.transform(full_feature)\n",
    "train, test = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"churn\", outputCol=\"label\")\n",
    "scaler = StandardScaler(inputCol=\"feature_vec\", outputCol=\"scaled_features\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression as Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc4da946f9c649dcaf7f24701458d72c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "lr =  LogisticRegression(maxIter=10, regParam=0.0, elasticNetParam=0, featuresCol='scaled_features')\n",
    "pipeline_logreg = Pipeline(stages=[indexer, scaler, lr])\n",
    "paramGrid = ParamGridBuilder().addGrid(lr.regParam,[0.0, 0.1, 1]).build()\n",
    "\n",
    "crossval_lr = CrossValidator(estimator=pipeline_logreg,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(metricName='f1'),\n",
    "                          numFolds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c74080adb3dd47eaa4f4e0cdae21e952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc7179df908a4f19be80eeb488fa998f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '400' from http://localhost:8998/sessions/1/statements/39 with error payload: {\"msg\":\"requirement failed: Session isn't active.\"}\n"
     ]
    }
   ],
   "source": [
    "cvModel_lr = crossval_lr.fit(train)\n",
    "print('F1 score for LogReg model in train :',  cvModel_lr.avgMetrics)\n",
    "results_lr = cvModel_lr.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20b3bde7e4544957aaaaebee533caa24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://localhost:8998/sessions/1 with error payload: {\"msg\":\"Session '1' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy for Logistic Regression Model in test: \", results_lr.filter(results_lr.label == results_lr.prediction).count()/ results_lr.count())\n",
    "evaluator = MulticlassClassificationEvaluator(metricName='f1')\n",
    "score = evaluator.evaluate(results_lr)\n",
    "print(\"F1 score for Logistic Regression model in test : \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b814532028f44d5aac457e3108c780b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'cvModel_lr' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'cvModel_lr' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_importances = cvModel_lr.bestModel.stages[-1].coefficients.values.tolist()\n",
    "\n",
    "def plot_feature(feature_importances, features_col):\n",
    "    feature_importance_df = pd.DataFrame({'coefficients': feature_importances, 'columns': features_col})\n",
    "    graph = sns.barplot(x='coefficients', y='columns', data=feature_importance_df.sort_values('coefficients',ascending=False))\n",
    "    graph.set_title('Feature coefficient plot')\n",
    "    \n",
    "    \n",
    "plot_feature(feature_importances,features_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27453808b90b405db4f358b7a3d7be72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"scaled_features\", maxIter=10)\n",
    "pipeline_gbt = Pipeline(stages=[indexer, scaler, gbt])\n",
    "paramGrid = ParamGridBuilder().build()\n",
    "\n",
    "crossval_gbt = CrossValidator(estimator=pipeline_gbt,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(metricName='f1'),\n",
    "                          numFolds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a6140508ea844e594930dad79caad6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bb80ed2a41f4354b9451a45463c4756",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-2275c8b5b0d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spark'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cvModel_gbt = crossval_gbt.fit(train)\\nprint('F1 score for GBT model in train :',  cvModel_gbt.avgMetrics)\\nresults_gbt = cvModel_gbt.transform(test)\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/emr/notebook-env/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2360\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2361\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2362\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2363\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-125>\u001b[0m in \u001b[0;36mspark\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/emr/notebook-env/lib/python3.7/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/emr/notebook-env/lib/python3.7/site-packages/sparkmagic/livyclientlib/exceptions.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_errors_are_fatal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/emr/notebook-env/lib/python3.7/site-packages/sparkmagic/livyclientlib/exceptions.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions_to_handle\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_errors_are_fatal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/emr/notebook-env/lib/python3.7/site-packages/sparkmagic/kernels/kernelmagics.py\u001b[0m in \u001b[0;36mspark\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0mcoerce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_coerce_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoerce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_spark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplemethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplefraction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoerce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/emr/notebook-env/lib/python3.7/site-packages/sparkmagic/magics/sparkmagicsbase.py\u001b[0m in \u001b[0;36mexecute_spark\u001b[0;34m(self, cell, output_var, samplemethod, maxrows, samplefraction, session_name, coerce)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexecute_spark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplemethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplefraction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoerce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmimetype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspark_controller\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown_session_on_spark_statement_errors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/emr/notebook-env/lib/python3.7/site-packages/sparkmagic/livyclientlib/sparkcontroller.py\u001b[0m in \u001b[0;36mrun_command\u001b[0;34m(self, command, client_name)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0msession_to_use\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_session_by_name_or_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcommand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession_to_use\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_sqlquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msqlquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/emr/notebook-env/lib/python3.7/site-packages/sparkmagic/livyclientlib/command.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, session)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_statement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mstatement_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mu'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_statement_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             self._spark_events.emit_statement_execution_end_event(session.guid, session.kind, session.id,\n",
      "\u001b[0;32m/emr/notebook-env/lib/python3.7/site-packages/sparkmagic/livyclientlib/command.py\u001b[0m in \u001b[0;36m_get_statement_output\u001b[0;34m(self, session, statement_id)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mFINAL_STATEMENT_STATUS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0mprogress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'progress'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                 \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m                 \u001b[0mretries\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/emr/notebook-env/lib/python3.7/site-packages/sparkmagic/livyclientlib/livysession.py\u001b[0m in \u001b[0;36msleep\u001b[0;34m(self, retries)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m         \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseconds_to_sleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;31m# This function will refresh the status and get the logs in a single call.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cvModel_gbt = crossval_gbt.fit(train)\n",
    "print('F1 score for GBT model in train :',  cvModel_gbt.avgMetrics)\n",
    "results_gbt = cvModel_gbt.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('F1 score for GBT model in train :',  cvModel_gbt.avgMetrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy for GBT Model in test: \", results_gbt.filter(results_gbt.label == results_gbt.prediction).count()/ results_gbt.count())\n",
    "evaluator = MulticlassClassificationEvaluator(metricName='f1')\n",
    "score = evaluator.evaluate(results_gbt)\n",
    "print(\"F1 score for GBT model in test : \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = cvModel_gbt.bestModel.stages[2].featureImportances.values.tolist()\n",
    "gbt_col = [features_col[x] for x in cvModel_gbt.bestModel.stages[2].featureImportances.indices.tolist()]\n",
    "\n",
    "plot_feature(feature_importances,gbt_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\",featuresCol='scaled_features')\n",
    "pipeline = Pipeline(stages=[indexer, scaler, nb])\n",
    "paramGrid = ParamGridBuilder().build()\n",
    "\n",
    "crossval_nb = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(metricName='f1'),\n",
    "                          numFolds=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel_nb = crossval_nb.fit(train)\n",
    "print('F1 score for Naive bayes model in train :',  cvModel_nb.avgMetrics)\n",
    "results_nb = cvModel_nb.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy for NB Model in test: \", results_nb.filter(results_nb.label == results_nb.prediction).count()/ results_nb.count())\n",
    "evaluator = MulticlassClassificationEvaluator(metricName='f1')\n",
    "score = evaluator.evaluate(results_nb)\n",
    "print(\"F1 score for NB model in test : \", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Steps\n",
    "Clean up your code, adding comments and renaming variables to make the code easier to read and maintain. Refer to the Spark Project Overview page and Data Scientist Capstone Project Rubric to make sure you are including all components of the capstone project and meet all expectations. Remember, this includes thorough documentation in a README file in a Github repository, as well as a web app or blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
