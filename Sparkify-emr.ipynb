{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify Project Workspace\n",
    "This workspace contains a tiny subset (128MB) of the full dataset available (12GB). Feel free to use this workspace to build your project, or to explore a smaller subset with Spark before deploying your cluster on the cloud. Instructions for setting up your Spark cluster is included in the last lesson of the Extracurricular Spark Course content.\n",
    "\n",
    "You can follow the steps below to guide your data analysis and model building portion of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'proxyUser': 'user_okids94', 'conf': {'spark.pyspark.python': 'python3', 'spark.pyspark.virtualenv.enabled': 'true', 'spark.pyspark.virtualenv.type': 'native', 'spark.pyspark.virtualenv.bin.path': '/usr/bin/virtualenv'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%info\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4de641507b247adb69124c09fccb521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1589627906482_0002</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-2-207.eu-west-1.compute.internal:20888/proxy/application_1589627906482_0002/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-12-72.eu-west-1.compute.internal:8042/node/containerlogs/container_1589627906482_0002_01_000002/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                    Version\n",
      "-------------------------- -------\n",
      "beautifulsoup4             4.9.0  \n",
      "boto                       2.49.0 \n",
      "jmespath                   0.9.5  \n",
      "lxml                       4.5.0  \n",
      "mysqlclient                1.4.2  \n",
      "nltk                       3.4.5  \n",
      "nose                       1.3.4  \n",
      "numpy                      1.16.5 \n",
      "pip                        9.0.1  \n",
      "py-dateutil                2.2    \n",
      "python37-sagemaker-pyspark 1.3.0  \n",
      "pytz                       2019.3 \n",
      "PyYAML                     5.3.1  \n",
      "setuptools                 28.8.0 \n",
      "six                        1.13.0 \n",
      "soupsieve                  1.9.5  \n",
      "wheel                      0.29.0 \n",
      "windmill                   1.6"
     ]
    }
   ],
   "source": [
    "sc.list_packages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9236b3c662641b0ab9fa3632fc7214c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1589646357143_0001</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-8-148.eu-west-1.compute.internal:20888/proxy/application_1589646357143_0001/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-4-136.eu-west-1.compute.internal:8042/node/containerlogs/container_1589646357143_0001_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas==0.25.1\n",
      "  Downloading https://files.pythonhosted.org/packages/7e/ab/ea76361f9d3e732e114adcd801d2820d5319c23d0ac5482fa3b412db217e/pandas-0.25.1-cp37-cp37m-manylinux1_x86_64.whl (10.4MB)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/site-packages (from pandas==0.25.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib64/python3.7/site-packages (from pandas==0.25.1)\n",
      "Collecting python-dateutil>=2.6.1 (from pandas==0.25.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/d4/70/d60450c3dd48ef87586924207ae8907090de0b306af2bce5d134d78615cb/python_dateutil-2.8.1-py2.py3-none-any.whl (227kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas==0.25.1)\n",
      "Installing collected packages: python-dateutil, pandas\n",
      "Successfully installed pandas-0.25.1 python-dateutil-2.8.1\n",
      "\n",
      "Collecting matplotlib\n",
      "  Downloading https://files.pythonhosted.org/packages/b2/c2/71fcf957710f3ba1f09088b35776a799ba7dd95f7c2b195ec800933b276b/matplotlib-3.2.1-cp37-cp37m-manylinux1_x86_64.whl (12.4MB)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /mnt/tmp/1589646587414-0/lib/python3.7/site-packages (from matplotlib)\n",
      "Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib)\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/bb/488841f56197b13700afd5658fc279a2025a39e22449b7cf29864669b15d/pyparsing-2.4.7-py2.py3-none-any.whl (67kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading https://files.pythonhosted.org/packages/f7/d2/e07d3ebb2bd7af696440ce7e754c59dd546ffe1bbe732c8ab68b9c834e61/cycler-0.10.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: numpy>=1.11 in /usr/local/lib64/python3.7/site-packages (from matplotlib)\n",
      "Collecting kiwisolver>=1.0.1 (from matplotlib)\n",
      "  Downloading https://files.pythonhosted.org/packages/31/b9/6202dcae729998a0ade30e80ac00f616542ef445b088ec970d407dfd41c0/kiwisolver-1.2.0-cp37-cp37m-manylinux1_x86_64.whl (88kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.1->matplotlib)\n",
      "Installing collected packages: pyparsing, cycler, kiwisolver, matplotlib\n",
      "Successfully installed cycler-0.10.0 kiwisolver-1.2.0 matplotlib-3.2.1 pyparsing-2.4.7\n",
      "\n",
      "Collecting httpagentparser\n",
      "  Downloading https://files.pythonhosted.org/packages/00/41/b7211f5aaf04e2952ca26e8e834bea3e76add2ae6ff7326449448a3164ea/httpagentparser-1.9.0.tar.gz\n",
      "Building wheels for collected packages: httpagentparser\n",
      "  Running setup.py bdist_wheel for httpagentparser: started\n",
      "  Running setup.py bdist_wheel for httpagentparser: finished with status 'done'\n",
      "  Stored in directory: /var/lib/livy/.cache/pip/wheels/87/aa/d6/3b45a743bf52f273cbc0efea22ebcd9347d9360eaefa4b8d49\n",
      "Successfully built httpagentparser\n",
      "Installing collected packages: httpagentparser\n",
      "Successfully installed httpagentparser-1.9.0\n",
      "\n",
      "Collecting seaborn\n",
      "  Downloading https://files.pythonhosted.org/packages/c7/e6/54aaaafd0b87f51dfba92ba73da94151aa3bc179e5fe88fc5dfb3038e860/seaborn-0.10.1-py3-none-any.whl (215kB)\n",
      "Requirement already satisfied: pandas>=0.22.0 in /mnt/tmp/1589646587414-0/lib/python3.7/site-packages (from seaborn)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib64/python3.7/site-packages (from seaborn)\n",
      "Collecting scipy>=1.0.1 (from seaborn)\n",
      "  Downloading https://files.pythonhosted.org/packages/dd/82/c1fe128f3526b128cfd185580ba40d01371c5d299fcf7f77968e22dfcc2e/scipy-1.4.1-cp37-cp37m-manylinux1_x86_64.whl (26.1MB)\n",
      "Requirement already satisfied: matplotlib>=2.1.2 in /mnt/tmp/1589646587414-0/lib/python3.7/site-packages (from seaborn)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/site-packages (from pandas>=0.22.0->seaborn)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /mnt/tmp/1589646587414-0/lib/python3.7/site-packages (from pandas>=0.22.0->seaborn)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /mnt/tmp/1589646587414-0/lib/python3.7/site-packages (from matplotlib>=2.1.2->seaborn)\n",
      "Requirement already satisfied: cycler>=0.10 in /mnt/tmp/1589646587414-0/lib/python3.7/site-packages (from matplotlib>=2.1.2->seaborn)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /mnt/tmp/1589646587414-0/lib/python3.7/site-packages (from matplotlib>=2.1.2->seaborn)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas>=0.22.0->seaborn)\n",
      "Installing collected packages: scipy, seaborn\n",
      "Successfully installed scipy-1.4.1 seaborn-0.10.1"
     ]
    }
   ],
   "source": [
    "sc.install_pypi_package(\"pandas==0.25.1\") #Install pandas version 0.25.1 \n",
    "sc.install_pypi_package(\"matplotlib\", \"https://pypi.org/simple\") #Install matplotlib from given PyPI repository\n",
    "sc.install_pypi_package(\"httpagentparser\")\n",
    "sc.install_pypi_package(\"seaborn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b339a0edc4d43729a9fe6340b073d82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "from pyspark.sql.functions import desc,asc,col,sum as Fsum,udf\n",
    "\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler,StringIndexer,StandardScaler\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression,GBTClassifier,NaiveBayes,RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "import datetime\n",
    "import httpagentparser\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# %matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "799cf82faaf848ecad533f3438a83f0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a Spark session\n",
    "\n",
    "# Create spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Sparkify\") \\\n",
    "    .getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Clean Dataset\n",
    "In this workspace, the mini-dataset file is `mini_sparkify_event_data.json`. Load and clean the dataset, checking for invalid or missing data - for example, records without userids or sessionids. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4380ef20f0434e80baa4bd6f8ed20244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read in full sparkify dataset\n",
    "event_data = \"s3n://udacity-dsnd/sparkify/sparkify_event_data.json\"\n",
    "df = spark.read.json(event_data)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, artist: string, auth: string, firstName: string, gender: string, itemInSession: string, lastName: string, length: string, level: string, location: string, method: string, page: string, registration: string, sessionId: string, song: string, status: string, ts: string, userAgent: string, userId: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "572bc76d60654a5a93f57b7e9b07706a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of dataset with null artist :  5408927\n",
      "number of dataset with null userid :  0\n",
      "number of dataset with null sessionid :  0\n",
      "number of dataset with empty userid :  0\n",
      "number of dataset with empty sessionid :  0"
     ]
    }
   ],
   "source": [
    "print('number of dataset with null artist : ' ,df.where(col(\"artist\").isNull()).count())\n",
    "print('number of dataset with null userid : ' ,df.where(col(\"userId\").isNull()).count())\n",
    "print('number of dataset with null sessionid : ' ,df.where(col(\"sessionId\").isNull()).count())\n",
    "print('number of dataset with empty userid : ' ,df.where(col(\"userId\") == '').count())\n",
    "print('number of dataset with empty sessionid : ' ,df.where(col(\"sessionid\") == '').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7a96b24ba5c40828ec49eb772cca84e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#filter empty string\n",
    "df_cleaned = df.filter(df.userId != '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b17e3a10d5104da393d35f6e5eaedcad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all data :  26259199\n",
      "non duplicate data :  26259199\n",
      "No duplicate"
     ]
    }
   ],
   "source": [
    "#Check duplicates\n",
    "print(\"all data : \",df_cleaned.count())\n",
    "print(\"non duplicate data : \", df_cleaned.drop_duplicates().count())\n",
    "if df_cleaned.count()==df_cleaned.drop_duplicates().count():\n",
    "    print('No duplicate')\n",
    "else:\n",
    "    print('duplicate dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "When you're working with the full dataset, perform EDA by loading a small subset of the data and doing basic manipulations within Spark. In this workspace, you are already provided a small subset of data you can explore.\n",
    "\n",
    "### Define Churn\n",
    "\n",
    "Once you've done some preliminary analysis, create a column `Churn` to use as the label for your model. I suggest using the `Cancellation Confirmation` events to define your churn, which happen for both paid and free users. As a bonus task, you can also look into the `Downgrade` events.\n",
    "\n",
    "### Explore Data\n",
    "Once you've defined churn, perform some exploratory data analysis to observe the behavior for users who stayed vs users who churned. You can start by exploring aggregates on these two groups of users, observing how much of a specific action they experienced per a certain time unit or number of songs played."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "884116cc1188436e9c9271a900726ca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|gender|   count|\n",
      "+------+--------+\n",
      "|  null|  778479|\n",
      "|     M|13299562|\n",
      "|     F|12181158|\n",
      "+------+--------+"
     ]
    }
   ],
   "source": [
    "#Distribution of gender\n",
    "df_cleaned.groupby('gender').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2692f7572484432fa40afc480e4932ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|           length|\n",
      "+-------+-----------------+\n",
      "|  count|         20850272|\n",
      "|   mean|248.7254329674883|\n",
      "| stddev|97.28710387078065|\n",
      "|    min|            0.522|\n",
      "|    max|       3024.66567|\n",
      "+-------+-----------------+"
     ]
    }
   ],
   "source": [
    "#Statistical result of song length\n",
    "df_cleaned.select('length').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a51d12f87a64717bc79d3c8c01807c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|                page|  count|\n",
      "+--------------------+-------+\n",
      "| Submit Registration|    401|\n",
      "|            Register|    802|\n",
      "|              Cancel|   5003|\n",
      "|Cancellation Conf...|   5003|\n",
      "|    Submit Downgrade|   6494|\n",
      "|      Submit Upgrade|  15135|\n",
      "|               Error|  25962|\n",
      "|       Save Settings|  29516|\n",
      "|             Upgrade|  50507|\n",
      "|               About|  92759|\n",
      "|            Settings| 147074|\n",
      "|                Help| 155100|\n",
      "|           Downgrade| 184240|\n",
      "|         Thumbs Down| 239212|\n",
      "|              Logout| 296005|\n",
      "|               Login| 296350|\n",
      "|          Add Friend| 381664|\n",
      "|         Roll Advert| 385212|\n",
      "|     Add to Playlist| 597921|\n",
      "|           Thumbs Up|1151465|\n",
      "+--------------------+-------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "df_cleaned.groupby('page').count().sort('count').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57a8ea095e4d48b4bd1201d5d131e8c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|status|   count|\n",
      "+------+--------+\n",
      "|   404|   25962|\n",
      "|   307| 2421245|\n",
      "|   200|23811992|\n",
      "+------+--------+"
     ]
    }
   ],
   "source": [
    "df_cleaned.groupby('status').count().sort('count').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f00f9d6838d440bc8558f466f94feaf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_cleaned.createOrReplaceTempView(\"event\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4caf827deb984ffdb1f579d1677a1416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------------+\n",
      "|gender|count(DISTINCT userId)|\n",
      "+------+----------------------+\n",
      "|     M|                  2656|\n",
      "|     F|                  2347|\n",
      "+------+----------------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT gender, COUNT(DISTINCT userId) \n",
    "             FROM event \n",
    "             WHERE page = 'Cancellation Confirmation' GROUP BY 1 \n",
    "          \"\"\").show(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total 5003 users is churning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "267c8246e8db4fcfb2b4dc535e9fd499",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|                page|count(1)|\n",
      "+--------------------+--------+\n",
      "|       Save Settings|       1|\n",
      "|     Add to Playlist|       2|\n",
      "|                Help|       1|\n",
      "|              Cancel|       1|\n",
      "|         Thumbs Down|       1|\n",
      "|                Home|      11|\n",
      "|         Roll Advert|       6|\n",
      "|             Upgrade|       3|\n",
      "|Cancellation Conf...|       1|\n",
      "|            NextSong|     123|\n",
      "|           Thumbs Up|       7|\n",
      "|              Logout|       3|\n",
      "|            Settings|       1|\n",
      "+--------------------+--------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT page, COUNT(*) FROM event WHERE userId in (\n",
    "             SELECT userId \n",
    "             FROM event \n",
    "             WHERE page = 'Cancellation Confirmation' LIMIT 1) GROUP BY 1 \n",
    "          \"\"\").show(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of list of page that churn user visited. If you see on the 'NextSong' page, this user played 123 songs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47734d99a5a146218674b20cc2c7774b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Create new view with churn columns\n",
    "event_with_churn = spark.sql(\"\"\"\n",
    "SELECT e.*, COALESCE(churn,0) AS churn\n",
    "FROM event e\n",
    "LEFT JOIN(\n",
    "        SELECT userid,1 AS churn\n",
    "         FROM event \n",
    "         WHERE page = 'Cancellation Confirmation'\n",
    "         GROUP BY 1\n",
    "         ) USING (userid)\n",
    "          \"\"\")\n",
    "event_with_churn.createOrReplaceTempView(\"event_with_churn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0cdb31d0a34429a8d170721e7310bec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------------+\n",
      "|churn|count(DISTINCT userId)|\n",
      "+-----+----------------------+\n",
      "|    1|                  5003|\n",
      "|    0|                 17275|\n",
      "+-----+----------------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" SELECT churn, COUNT(DISTINCT userId) FROM\n",
    "event_with_churn\n",
    "GROUP BY 1\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb2ab286a1704e278fee40f1c6522757",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+-----------------------+------------------+----------------------+------------------+------------------+\n",
      "|churn|  avg_song_played|avg_song_per_day_played|median_song_played|avg_unique_song_played|      unique_ratio|          user_age|\n",
      "+-----+-----------------+-----------------------+------------------+----------------------+------------------+------------------+\n",
      "|    1|876.7291625024985|        58.727684128275|               512|     779.5772536478113| 1.124621271849701|3.7419776765812545|\n",
      "|    0|953.0533140376266|      53.24271634785819|               563|     842.7589001447178|1.1308730336445798| 5.231680025687944|\n",
      "+-----+-----------------+-----------------------+------------------+----------------------+------------------+------------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" \n",
    "SELECT churn, \n",
    "       AVG(cnt_song) avg_song_played,\n",
    "       AVG(cnt_song/cnt_day_played) avg_song_per_day_played,\n",
    "       percentile_approx(cnt_song, 0.5) median_song_played,\n",
    "       AVG(cnt_unique_song)  avg_unique_song_played,\n",
    "       AVG(cnt_song)/AVG(cnt_unique_song) AS unique_ratio,\n",
    "       AVG(ts-registration)/(1000*3600*365*24) AS user_age\n",
    " FROM\n",
    "(SELECT userId, \n",
    "        MAX(churn) AS churn, \n",
    "        SUM(CASE WHEN page = 'NextSong' THEN 1 ELSE 0 END) cnt_song,\n",
    "        COUNT(DISTINCT CASE WHEN page = 'NextSong' THEN song END) cnt_unique_song,\n",
    "        MAX(ts) ts,\n",
    "        COUNT(DISTINCT CASE WHEN page = 'NextSong' THEN from_unixtime(ts/1000, 'yyyy-MM-dd') ELSE 0 END) cnt_day_played,\n",
    "        MAX(registration) registration\n",
    "   FROM event_with_churn\n",
    " GROUP BY 1) GROUP BY 1 \n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On average, churn user played less song compared to active user. They also have less in user_age (3.7 years compare to 5.2 years). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80c7c30461c44a15af51fb4df8ab1d53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "|level|count(1)|\n",
      "+-----+--------+\n",
      "| free| 5663371|\n",
      "| paid|20595828|\n",
      "+-----+--------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT level, COUNT(*) FROM event_with_churn GROUP BY 1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30d220ed2ad54a388cf24eae830d1ea9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_song_perday = spark.sql(\"\"\" \n",
    "SELECT churn, \n",
    "       cnt_song/cnt_day_played num_song_perday\n",
    " FROM\n",
    "(SELECT userId, \n",
    "        MAX(churn) AS churn, \n",
    "        SUM(CASE WHEN page = 'NextSong' THEN 1 ELSE 0 END) cnt_song,\n",
    "        MAX(ts) ts,\n",
    "        COUNT(DISTINCT CASE WHEN page = 'NextSong' THEN from_unixtime(ts/1000, 'yyyy-MM-dd') ELSE 0 END) cnt_day_played,\n",
    "        MAX(registration) registration\n",
    "   FROM event_with_churn\n",
    " GROUP BY 1) \n",
    "\"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41d494da8dc2470483a6ea639d5cbe4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<matplotlib.axes._subplots.AxesSubplot object at 0x7f98d9e5d990>"
     ]
    }
   ],
   "source": [
    "df_song_perday[df_song_perday.churn == 1].num_song_perday.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29cd3cff5e0d439391902d2194167bcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<matplotlib.axes._subplots.AxesSubplot object at 0x7f98d9e5d990>"
     ]
    }
   ],
   "source": [
    "df_song_perday[df_song_perday.churn == 0].num_song_perday.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Churn user has right skewed distribution in terms of song per day, which means they play less song per day. On the other hand, active users is more similar to normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Once you've familiarized yourself with the data, build out the features you find promising to train your model on. To work with the full dataset, you can follow the following steps.\n",
    "- Write a script to extract the necessary features from the smaller subset of data\n",
    "- Ensure that your script is scalable, using the best practices discussed in Lesson 3\n",
    "- Try your script on the full data set, debugging your script if necessary\n",
    "\n",
    "If you are working in the classroom workspace, you can just extract features based on the small subset of data contained here. Be sure to transfer over this work to the larger dataset when you work on your Spark cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2677d1e1fbfa49b98ee3e47589aa6bf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Create dummy column\n",
    "def create_dummy_columns(col_name,df):\n",
    "    '''\n",
    "    Input :\n",
    "    - col_name = name of the column that is used to generate dummy variables\n",
    "    - df = spark df\n",
    "    Output :\n",
    "    - ndf = new spark df with all dummy variables from selected column\n",
    "    '''\n",
    "    elements = df.select(col_name).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "    exprs = [F.when(F.col(col_name) == element, 1).otherwise(0).alias(element)\n",
    "         for element in elements]\n",
    "    df_with_dummy = df.select(\"*\", *exprs)\n",
    "    \n",
    "    #drop original column\n",
    "    ndf = df_with_dummy.drop(col_name)\n",
    "    return ndf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3fc8ecbc03545b59670f9ee39316b21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_with_dummy_pages = create_dummy_columns('page',event_with_churn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee95294836334e869210a0a253bc1db5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_browser(x):\n",
    "    try :\n",
    "        r = httpagentparser.detect(x)['browser']['name']\n",
    "    except:\n",
    "        r = ''\n",
    "    return r\n",
    "    \n",
    "def get_platform(x):\n",
    "    try :\n",
    "        r = httpagentparser.detect(x)['platform']['name']\n",
    "    except:\n",
    "        r = ''\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62ee00416e924d63a4da2ff6413a7403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function get_platform at 0x7f719c091dd0>"
     ]
    }
   ],
   "source": [
    "#browser\n",
    "spark.udf.register(\"get_browser\", get_browser)\n",
    "\n",
    "#Platform\n",
    "spark.udf.register(\"get_platform\", get_platform)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check unique combination\n",
    "spark.sql(\"SELECT get_browser(userAgent), get_platform(userAgent) FROM event_with_churn GROUP BY 1,2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f6f8403bfa44c31a7b8ac8eacf80fb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Crafting session level feature\n",
    "session_feature = spark.sql(\"\"\"\n",
    "SELECT userId, \n",
    "       AVG(hour_per_session)             AS avg_hour_per_session,\n",
    "       AVG(unique_song_per_session)      AS avg_unique_song_per_session,\n",
    "       AVG(unique_artist_per_session)    AS avg_unique_artist_per_session,\n",
    "       AVG(cnt_song_played_persession)   AS avg_song_played_persession\n",
    "FROM (       \n",
    "SELECT userId,\n",
    "       sessionId,\n",
    "       (MAX(ts) -MIN(ts))/(1000*3600) AS hour_per_session,\n",
    "       COUNT(DISTINCT song) unique_song_per_session,\n",
    "       COUNT(DISTINCT artist) unique_artist_per_session,\n",
    "       SUM(CASE WHEN page = 'NextSong' THEN 1 ELSE 0 END) AS cnt_song_played_persession\n",
    "FROM event_with_churn \n",
    "GROUP BY 1,2)\n",
    "GROUP BY 1\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4346f6a9a755458aaf2ddfa348d1e69a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+---------------------------+-----------------------------+--------------------------+\n",
      "| userId|avg_hour_per_session|avg_unique_song_per_session|avg_unique_artist_per_session|avg_song_played_persession|\n",
      "+-------+--------------------+---------------------------+-----------------------------+--------------------------+\n",
      "|1338783|  7.9227951388888895|                     101.75|                      95.8125|                  104.5625|\n",
      "|1271218|   6.910691721132898|          91.76470588235294|            87.07843137254902|         93.70588235294117|\n",
      "|1396135|   7.612685185185185|         105.88888888888889|                        101.0|        108.22222222222223|\n",
      "|1690101|   4.364001501501501|         63.513513513513516|            61.37837837837838|         64.37837837837837|\n",
      "|1676292|  10.208055555555553|         144.72727272727272|           132.45454545454547|         149.0909090909091|\n",
      "|1380035|  5.4878806584362145|          71.25925925925925|            69.14814814814815|         72.07407407407408|\n",
      "|1732896|   7.269055555555555|                      101.6|                         97.8|                     102.8|\n",
      "|1440693|  1.3508854166666666|                    20.6875|                      20.4375|                    20.875|\n",
      "|1507765|   4.745333333333333|                       68.4|                         65.8|                      69.4|\n",
      "|1853227|  3.2821450617283947|         47.888888888888886|            46.05555555555556|         48.77777777777778|\n",
      "|1242455|   6.856421568627452|          96.17647058823529|                         91.0|         98.47058823529412|\n",
      "|1010669|   9.027962962962963|         121.47619047619048|            115.0952380952381|         124.0952380952381|\n",
      "|1927014|   5.690260942760942|          76.27272727272727|            72.54545454545455|         77.81818181818181|\n",
      "|1675930|   6.767703703703703|          96.26666666666667|            91.66666666666667|                      98.0|\n",
      "|1925919|   1.336527777777778|                       21.5|                         21.5|                      21.5|\n",
      "|1883991|   7.613842592592591|                     104.75|            96.41666666666667|        109.08333333333333|\n",
      "|1025974|   4.239823232323233|          61.81818181818182|            59.72727272727273|                      62.0|\n",
      "|1297189|  3.5607142857142855|          51.57142857142857|           50.142857142857146|                      52.0|\n",
      "|1766909|   2.090099206349206|         30.857142857142858|           30.571428571428573|        30.928571428571427|\n",
      "|1008404|   6.527443415637861|          91.01851851851852|            86.37037037037037|         92.87037037037037|\n",
      "+-------+--------------------+---------------------------+-----------------------------+--------------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "session_feature.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ebb40fa8dec4122a62f062a2d626253",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "session_feature.createOrReplaceTempView('session_feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03bf04cfe7994f26b4557d6c897feaae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Crafting the feature\n",
    "full_feature = spark.sql(\"\"\"\n",
    "SELECT userId,\n",
    "       churn,  \n",
    "       --Gender\n",
    "       MAX(CASE WHEN gender = 'M' THEN 1 ELSE 0 END)  AS gender_male,\n",
    "       MAX(CASE WHEN gender = 'F' THEN 1 ELSE 0 END) AS gender_female,\n",
    "       \n",
    "       -- browser\n",
    "       MAX(CASE WHEN get_browser(userAgent) = 'Chrome' THEN 1 ELSE 0 END)  AS chrome_user,\n",
    "       MAX(CASE WHEN get_browser(userAgent) = 'Firefox' THEN 1 ELSE 0 END)  AS firefox_user,\n",
    "       MAX(CASE WHEN get_browser(userAgent) = 'Safari' THEN 1 ELSE 0 END)  AS safari_user,\n",
    "       MAX(CASE WHEN get_browser(userAgent) LIKE '%Microsoft%' THEN 1 ELSE 0 END)  AS ie_user,\n",
    "       \n",
    "       --platform\n",
    "       --MAX(CASE WHEN get_platform(userAgent) = 'Linux' THEN 1 ELSE 0 END)  AS linux_user,\n",
    "       --MAX(CASE WHEN get_platform(userAgent) = 'iOS' THEN 1 ELSE 0 END)  AS ios_user,\n",
    "       --MAX(CASE WHEN get_platform(userAgent) = 'Mac OS' THEN 1 ELSE 0 END)  AS mac_user,\n",
    "       --MAX(CASE WHEN get_platform(userAgent) = 'Windows' THEN 1 ELSE 0 END)  AS windows_user,\n",
    "       \n",
    "       \n",
    "       \n",
    "       \n",
    "       -- Paid and free session\n",
    "       COUNT(DISTINCT CASE WHEN level = 'paid' THEN sessionId ELSE 0 END) AS cnt_paid_session,\n",
    "       COUNT(DISTINCT CASE WHEN level = 'free' THEN sessionId ELSE 0 END) AS cnt_free_session,\n",
    "       \n",
    "       -- Artist and song behavior\n",
    "       --COUNT(DISTINCT artist) AS cnt_unique_artist,\n",
    "       COUNT(DISTINCT song) AS cnt_unique_song,\n",
    "       --AVG(length) AS average_song_length,\n",
    "       SUM(CASE WHEN page = 'NextSong' THEN 1 ELSE 0 END) AS cnt_song_played,\n",
    "    \n",
    "       -- Other behavior\n",
    "       COUNT(DISTINCT CASE WHEN page = 'NextSong' THEN from_unixtime(ts/1000, 'yyyy-MM-dd') ELSE 0 END) AS cnt_day_played,\n",
    "       COUNT(DISTINCT sessionId) AS cnt_session,\n",
    "       SUM(CASE WHEN page = 'Add Friend' THEN 1 ELSE 0 END) AS cnt_add_friend,\n",
    "       SUM(CASE WHEN page = 'Thumbs Down' THEN 1 ELSE 0 END) AS cnt_thumb_down,\n",
    "       SUM(CASE WHEN page = 'Thumbs Up' THEN 1 ELSE 0 END) AS cnt_thumbs_up,\n",
    "       SUM(CASE WHEN page = 'Home' THEN 1 ELSE 0 END) AS cnt_home,\n",
    "       --SUM(CASE WHEN page = 'Logout' THEN 1 ELSE 0 END) AS cnt_logout,\n",
    "       --SUM(CASE WHEN page = 'Help' THEN 1 ELSE 0 END) AS cnt_help,\n",
    "       --SUM(CASE WHEN page = 'Settings' THEN 1 ELSE 0 END) AS cnt_settings,\n",
    "       --SUM(CASE WHEN page = 'Submit Downgrade' THEN 1 ELSE 0 END) AS cnt_submit_downgrade,\n",
    "       --SUM(CASE WHEN page = 'Submit Upgrade' THEN 1 ELSE 0 END) AS cnt_submit_upgrade,\n",
    "       --SUM(CASE WHEN page = 'Downgrade' THEN 1 ELSE 0 END) AS cnt_initiate_downgrade,\n",
    "       --SUM(CASE WHEN page = 'Upgrade' THEN 1 ELSE 0 END) AS cnt_initiate_upgrade,\n",
    "       SUM(CASE WHEN page = 'Error' THEN 1 ELSE 0 END) AS cnt_error,\n",
    "       MAX(ts-registration)/(1000*3600*365*24) AS user_age\n",
    "       \n",
    "       --Combining session feature\n",
    "       --MAX(sf.avg_hour_per_session)  AS avg_hour_per_session,\n",
    "       --MAX(sf.avg_unique_song_per_session) AS avg_unique_song_per_session,\n",
    "       --MAX(sf.avg_unique_artist_per_session) AS avg_unique_artist_per_session,\n",
    "       --MAX(sf.avg_song_played_persession) AS avg_song_played_persession\n",
    "    \n",
    "FROM event_with_churn \n",
    "--LEFT JOIN session_feature sf USING (userId)\n",
    "GROUP BY 1,2     \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e680637126f4ac2970a7cff54206e63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[userId: string, churn: int, gender_male: int, gender_female: int, chrome_user: int, firefox_user: int, safari_user: int, ie_user: int, cnt_paid_session: bigint, cnt_free_session: bigint, cnt_unique_song: bigint, cnt_song_played: bigint, cnt_day_played: bigint, cnt_session: bigint, cnt_add_friend: bigint, cnt_thumb_down: bigint, cnt_thumbs_up: bigint, cnt_home: bigint, cnt_error: bigint, user_age: double]"
     ]
    }
   ],
   "source": [
    "full_feature.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "Split the full dataset into train, test, and validation sets. Test out several of the machine learning methods you learned. Evaluate the accuracy of the various models, tuning parameters as necessary. Determine your winning model based on test accuracy and report results on the validation set. Since the churned users are a fairly small subset, I suggest using F1 score as the metric to optimize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only selected features are used in this notebook compare to smaller dataseat to overcome EMR performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5f8c09f4b8847d5b4be675a65a4e3cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "features_col = ['gender_male','gender_female','chrome_user','firefox_user','safari_user',\n",
    "               'ie_user',\n",
    "                #'linux_user','ios_user','mac_user','windows_user','cnt_paid_session',\n",
    "               'cnt_free_session',#'cnt_unique_artist',\n",
    "               'cnt_unique_song',\n",
    "                #'average_song_length',\n",
    "               'cnt_song_played','cnt_day_played','cnt_session','cnt_add_friend','cnt_thumb_down',\n",
    "               'cnt_thumbs_up','cnt_home',\n",
    "                #'cnt_logout','cnt_help','cnt_settings','cnt_submit_downgrade',\n",
    "                #'cnt_submit_upgrade','cnt_initiate_downgrade','cnt_initiate_upgrade',\n",
    "               'cnt_error','user_age'\n",
    "                #,'avg_hour_per_session',\n",
    "                #'avg_unique_song_per_session','avg_unique_artist_per_session'\n",
    "                ]\n",
    "assembler = VectorAssembler(inputCols=features_col, outputCol=\"feature_vec\")\n",
    "\n",
    "df = assembler.setHandleInvalid(\"skip\").transform(full_feature)\n",
    "train, test = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"churn\", outputCol=\"label\")\n",
    "scaler = StandardScaler(inputCol=\"feature_vec\", outputCol=\"scaled_features\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression as Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "483600ca97a4433caef1d917b21a8b60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "lr =  LogisticRegression(maxIter=10, regParam=0.0, elasticNetParam=0, featuresCol='scaled_features')\n",
    "pipeline_logreg = Pipeline(stages=[indexer, scaler, lr])\n",
    "paramGrid = ParamGridBuilder().addGrid(lr.regParam,[0.0, 0.1, 1]).build()\n",
    "\n",
    "crossval_lr = CrossValidator(estimator=pipeline_logreg,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(metricName='f1'),\n",
    "                          numFolds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel_lr = crossval_lr.fit(train)\n",
    "print('F1 score for LogReg model in train :',  cvModel_lr.avgMetrics)\n",
    "results_lr = cvModel_lr.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a63af54473774b32a72085215e26125c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Logistic Regression Model in test:  0.8212699822380106\n",
      "F1 score for Logistic Regression model in test :  0.7803118352464993"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy for Logistic Regression Model in test: \", results_lr.filter(results_lr.label == results_lr.prediction).count()/ results_lr.count())\n",
    "evaluator = MulticlassClassificationEvaluator(metricName='f1')\n",
    "score = evaluator.evaluate(results_lr)\n",
    "print(\"F1 score for Logistic Regression model in test : \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d7e4e4010084083865aa642859d7e65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_importances = cvModel_lr.bestModel.stages[-1].coefficients.values.tolist()\n",
    "\n",
    "def plot_feature(feature_importances, features_col):\n",
    "    feature_importance_df = pd.DataFrame({'coefficients': feature_importances, 'columns': features_col})\n",
    "    graph = sns.barplot(x='coefficients', y='columns', data=feature_importance_df.sort_values('coefficients',ascending=False))\n",
    "    graph.set_title('Feature coefficient plot')\n",
    "    \n",
    "    \n",
    "plot_feature(feature_importances,features_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39340a5141fd40dda693e9334552feeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"scaled_features\", maxIter=10)\n",
    "pipeline_gbt = Pipeline(stages=[indexer, scaler, gbt])\n",
    "paramGrid = ParamGridBuilder().build()\n",
    "\n",
    "crossval_gbt = CrossValidator(estimator=pipeline_gbt,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(metricName='f1'),\n",
    "                          numFolds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3cbfbc0b08e4aa891b1c33b41c9fc1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread cell_monitor-31:\n",
      "Traceback (most recent call last):\n",
      "  File \"/emr/notebook-env/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/emr/notebook-env/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/emr/notebook-env/lib/python3.7/site-packages/awseditorssparkmonitoringwidget-1.0-py3.7.egg/awseditorssparkmonitoringwidget/cellmonitor.py\", line 178, in cell_monitor\n",
      "    job_binned_stages[job_id][stage_id] = all_stages[stage_id]\n",
      "KeyError: 2346\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for GBT model in train : [0.8077054267756896]"
     ]
    }
   ],
   "source": [
    "cvModel_gbt = crossval_gbt.fit(train)\n",
    "print('F1 score for GBT model in train :',  cvModel_gbt.avgMetrics)\n",
    "results_gbt = cvModel_gbt.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10e7bd5b3ab045beb6a7b327f3a5074b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for GBT model in train : [0.8077054267756896]"
     ]
    }
   ],
   "source": [
    "print('F1 score for GBT model in train :',  cvModel_gbt.avgMetrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd95d6959f784281ab7034ca28d7d45d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for GBT Model in test:  0.8441385435168739\n",
      "F1 score for GBT model in test :  0.828449317082134"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy for GBT Model in test: \", results_gbt.filter(results_gbt.label == results_gbt.prediction).count()/ results_gbt.count())\n",
    "evaluator = MulticlassClassificationEvaluator(metricName='f1')\n",
    "score = evaluator.evaluate(results_gbt)\n",
    "print(\"F1 score for GBT model in test : \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02abc328a284462bb6082c3021d7051e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_importances = cvModel_gbt.bestModel.stages[2].featureImportances.values.tolist()\n",
    "gbt_col = [features_col[x] for x in cvModel_gbt.bestModel.stages[2].featureImportances.indices.tolist()]\n",
    "\n",
    "plot_feature(feature_importances,gbt_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "748a18fa53c7402793e4f4904e9cdcc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\",featuresCol='scaled_features')\n",
    "pipeline = Pipeline(stages=[indexer, scaler, nb])\n",
    "paramGrid = ParamGridBuilder().build()\n",
    "\n",
    "crossval_nb = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(metricName='f1'),\n",
    "                          numFolds=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffacf649387c46b987f9f267c1991d20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o6297.fit.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 16 in stage 3184.0 failed 4 times, most recent failure: Lost task 16.3 in stage 3184.0 (TID 153850, ip-172-31-4-136.eu-west-1.compute.internal, executor 7): java.lang.IllegalArgumentException: requirement failed: Naive Bayes requires nonnegative feature values but found [0.0,2.0016086174146306,0.0,0.0,2.5713021052493215,0.0,0.41638696427841526,0.05860759316046543,0.047807726206852856,0.3744267831483405,0.2307709434337849,0.04908113282955325,0.0,0.06171763548007865,0.022030137244053144,0.0,-1.8829414639316775E-4].\n",
      "\tat scala.Predef$.require(Predef.scala:224)\n",
      "\tat org.apache.spark.ml.classification.NaiveBayes$.requireNonnegativeValues(NaiveBayes.scala:235)\n",
      "\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1$$anonfun$4.apply(NaiveBayes.scala:144)\n",
      "\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1$$anonfun$4.apply(NaiveBayes.scala:144)\n",
      "\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1$$anonfun$7.apply(NaiveBayes.scala:168)\n",
      "\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1$$anonfun$7.apply(NaiveBayes.scala:166)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$$anonfun$5.apply(ExternalSorter.scala:190)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$$anonfun$5.apply(ExternalSorter.scala:189)\n",
      "\tat org.apache.spark.util.collection.AppendOnlyMap.changeValue(AppendOnlyMap.scala:150)\n",
      "\tat org.apache.spark.util.collection.SizeTrackingAppendOnlyMap.changeValue(SizeTrackingAppendOnlyMap.scala:32)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:195)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:64)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2043)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2031)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2030)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2030)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:967)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:967)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:967)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2264)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2213)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2202)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:778)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n",
      "\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1.apply(NaiveBayes.scala:176)\n",
      "\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1.apply(NaiveBayes.scala:129)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)\n",
      "\tat scala.util.Try$.apply(Try.scala:192)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)\n",
      "\tat org.apache.spark.ml.classification.NaiveBayes.trainWithLabelCheck(NaiveBayes.scala:129)\n",
      "\tat org.apache.spark.ml.classification.NaiveBayes.train(NaiveBayes.scala:118)\n",
      "\tat org.apache.spark.ml.classification.NaiveBayes.train(NaiveBayes.scala:78)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:82)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.lang.IllegalArgumentException: requirement failed: Naive Bayes requires nonnegative feature values but found [0.0,2.0016086174146306,0.0,0.0,2.5713021052493215,0.0,0.41638696427841526,0.05860759316046543,0.047807726206852856,0.3744267831483405,0.2307709434337849,0.04908113282955325,0.0,0.06171763548007865,0.022030137244053144,0.0,-1.8829414639316775E-4].\n",
      "\tat scala.Predef$.require(Predef.scala:224)\n",
      "\tat org.apache.spark.ml.classification.NaiveBayes$.requireNonnegativeValues(NaiveBayes.scala:235)\n",
      "\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1$$anonfun$4.apply(NaiveBayes.scala:144)\n",
      "\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1$$anonfun$4.apply(NaiveBayes.scala:144)\n",
      "\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1$$anonfun$7.apply(NaiveBayes.scala:168)\n",
      "\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1$$anonfun$7.apply(NaiveBayes.scala:166)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$$anonfun$5.apply(ExternalSorter.scala:190)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$$anonfun$5.apply(ExternalSorter.scala:189)\n",
      "\tat org.apache.spark.util.collection.AppendOnlyMap.changeValue(AppendOnlyMap.scala:150)\n",
      "\tat org.apache.spark.util.collection.SizeTrackingAppendOnlyMap.changeValue(SizeTrackingAppendOnlyMap.scala:32)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:195)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:64)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/base.py\", line 132, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/tuning.py\", line 304, in _fit\n",
      "    for j, metric, subModel in pool.imap_unordered(lambda f: f(), tasks):\n",
      "  File \"/usr/lib64/python3.7/multiprocessing/pool.py\", line 748, in next\n",
      "    raise value\n",
      "  File \"/usr/lib64/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/tuning.py\", line 304, in <lambda>\n",
      "    for j, metric, subModel in pool.imap_unordered(lambda f: f(), tasks):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/tuning.py\", line 52, in singleTask\n",
      "    index, model = next(modelIter)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/base.py\", line 62, in __next__\n",
      "    return index, self.fitSingleModel(index)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/base.py\", line 106, in fitSingleModel\n",
      "    return estimator.fit(dataset, paramMaps[index])\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/base.py\", line 132, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/pipeline.py\", line 109, in _fit\n",
      "    model = stage.fit(dataset)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/base.py\", line 132, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/wrapper.py\", line 295, in _fit\n",
      "    java_model = self._fit_java(dataset)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/wrapper.py\", line 292, in _fit_java\n",
      "    return self._java_obj.fit(dataset._jdf)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o6297.fit.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 16 in stage 3184.0 failed 4 times, most recent failure: Lost task 16.3 in stage 3184.0 (TID 153850, ip-172-31-4-136.eu-west-1.compute.internal, executor 7): java.lang.IllegalArgumentException: requirement failed: Naive Bayes requires nonnegative feature values but found [0.0,2.0016086174146306,0.0,0.0,2.5713021052493215,0.0,0.41638696427841526,0.05860759316046543,0.047807726206852856,0.3744267831483405,0.2307709434337849,0.04908113282955325,0.0,0.06171763548007865,0.022030137244053144,0.0,-1.8829414639316775E-4].\n",
      "\tat scala.Predef$.require(Predef.scala:224)\n",
      "\tat org.apache.spark.ml.classification.NaiveBayes$.requireNonnegativeValues(NaiveBayes.scala:235)\n",
      "\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1$$anonfun$4.apply(NaiveBayes.scala:144)\n",
      "\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1$$anonfun$4.apply(NaiveBayes.scala:144)\n",
      "\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1$$anonfun$7.apply(NaiveBayes.scala:168)\n",
      "\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1$$anonfun$7.apply(NaiveBayes.scala:166)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$$anonfun$5.apply(ExternalSorter.scala:190)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$$anonfun$5.apply(ExternalSorter.scala:189)\n",
      "\tat org.apache.spark.util.collection.AppendOnlyMap.changeValue(AppendOnlyMap.scala:150)\n",
      "\tat org.apache.spark.util.collection.SizeTrackingAppendOnlyMap.changeValue(SizeTrackingAppendOnlyMap.scala:32)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:195)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:64)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2043)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2031)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2030)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2030)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:967)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:967)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:967)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2264)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2213)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2202)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:778)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n",
      "\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1.apply(NaiveBayes.scala:176)\n",
      "\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1.apply(NaiveBayes.scala:129)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)\n",
      "\tat scala.util.Try$.apply(Try.scala:192)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)\n",
      "\tat org.apache.spark.ml.classification.NaiveBayes.trainWithLabelCheck(NaiveBayes.scala:129)\n",
      "\tat org.apache.spark.ml.classification.NaiveBayes.train(NaiveBayes.scala:118)\n",
      "\tat org.apache.spark.ml.classification.NaiveBayes.train(NaiveBayes.scala:78)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:82)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.lang.IllegalArgumentException: requirement failed: Naive Bayes requires nonnegative feature values but found [0.0,2.0016086174146306,0.0,0.0,2.5713021052493215,0.0,0.41638696427841526,0.05860759316046543,0.047807726206852856,0.3744267831483405,0.2307709434337849,0.04908113282955325,0.0,0.06171763548007865,0.022030137244053144,0.0,-1.8829414639316775E-4].\n",
      "\tat scala.Predef$.require(Predef.scala:224)\n",
      "\tat org.apache.spark.ml.classification.NaiveBayes$.requireNonnegativeValues(NaiveBayes.scala:235)\n",
      "\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1$$anonfun$4.apply(NaiveBayes.scala:144)\n",
      "\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1$$anonfun$4.apply(NaiveBayes.scala:144)\n",
      "\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1$$anonfun$7.apply(NaiveBayes.scala:168)\n",
      "\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1$$anonfun$7.apply(NaiveBayes.scala:166)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$$anonfun$5.apply(ExternalSorter.scala:190)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$$anonfun$5.apply(ExternalSorter.scala:189)\n",
      "\tat org.apache.spark.util.collection.AppendOnlyMap.changeValue(AppendOnlyMap.scala:150)\n",
      "\tat org.apache.spark.util.collection.SizeTrackingAppendOnlyMap.changeValue(SizeTrackingAppendOnlyMap.scala:32)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:195)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:64)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cvModel_nb = crossval_nb.fit(train)\n",
    "print('F1 score for Naive bayes model in train :',  cvModel_nb.avgMetrics)\n",
    "results_nb = cvModel_nb.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9508ce2bc824b6cb1a9fd93d4b9120f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'results_nb' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'results_nb' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy for NB Model in test: \", results_nb.filter(results_nb.label == results_nb.prediction).count()/ results_nb.count())\n",
    "evaluator = MulticlassClassificationEvaluator(metricName='f1')\n",
    "score = evaluator.evaluate(results_nb)\n",
    "print(\"F1 score for NB model in test : \", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37da1fe3321647f4a26c44c11ff40f23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"scaled_features\", numTrees=10)\n",
    "pipeline = Pipeline(stages=[indexer, scaler, rf])\n",
    "paramGrid = ParamGridBuilder().build()\n",
    "\n",
    "crossval_rf = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(metricName='f1'),\n",
    "                          numFolds=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "028161075d2f4bee886e94dc5282a297",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread cell_monitor-42:\n",
      "Traceback (most recent call last):\n",
      "  File \"/emr/notebook-env/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/emr/notebook-env/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/emr/notebook-env/lib/python3.7/site-packages/awseditorssparkmonitoringwidget-1.0-py3.7.egg/awseditorssparkmonitoringwidget/cellmonitor.py\", line 178, in cell_monitor\n",
      "    job_binned_stages[job_id][stage_id] = all_stages[stage_id]\n",
      "KeyError: 3290\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for Random Forrest model in train : [0.8023975974731592]"
     ]
    }
   ],
   "source": [
    "cvModel_rf = crossval_rf.fit(train)\n",
    "print('F1 score for Random Forrest model in train :',  cvModel_rf.avgMetrics)\n",
    "results_rf = cvModel_rf.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22cb415a3f584f62910c97d5f7851ece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread cell_monitor-43:\n",
      "Traceback (most recent call last):\n",
      "  File \"/emr/notebook-env/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/emr/notebook-env/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/emr/notebook-env/lib/python3.7/site-packages/awseditorssparkmonitoringwidget-1.0-py3.7.egg/awseditorssparkmonitoringwidget/cellmonitor.py\", line 178, in cell_monitor\n",
      "    job_binned_stages[job_id][stage_id] = all_stages[stage_id]\n",
      "KeyError: 3493\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Random Forrest Model in test:  0.8368117229129662\n",
      "F1 score for Random Forrest model in test :  0.8184387982397876"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy for Random Forrest Model in test: \", results_rf.filter(results_rf.label == results_rf.prediction).count()/ results_rf.count())\n",
    "evaluator = MulticlassClassificationEvaluator(metricName='f1')\n",
    "score = evaluator.evaluate(results_rf)\n",
    "print(\"F1 score for Random Forrest model in test : \", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In full dataset, Random forrest has the highest F1 score compare to others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Steps\n",
    "Clean up your code, adding comments and renaming variables to make the code easier to read and maintain. Refer to the Spark Project Overview page and Data Scientist Capstone Project Rubric to make sure you are including all components of the capstone project and meet all expectations. Remember, this includes thorough documentation in a README file in a Github repository, as well as a web app or blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
